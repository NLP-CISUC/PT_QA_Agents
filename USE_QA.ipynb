{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ_JwPCM0jC-"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Universal Sentence Encoder Q&A Retrieval\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTDnVj8k0RkP"
      },
      "outputs": [],
      "source": [
        "# receives\n",
        "# .txt file containing a list of question-answer pairs identified by 'P: ' and 'R: '\n",
        "# .txt file with a list of questions, one question per line\n",
        "\n",
        "# retrieves\n",
        "# .txt file containing the posed questions and respective useQA's answers, identified by 'P: ' and 'R: '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSSb6hR90wSg"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"tensorflow-text==2.8.*\"\n",
        "!pip install -q simpleneighbors[annoy]\n",
        "!pip install -q nltk\n",
        "!pip install -q tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf6YWgti1E_I"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import simpleneighbors\n",
        "import urllib\n",
        "from IPython.display import HTML, display\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_text import SentencepieceTokenizer\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVv63sAg1dUk"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\"\n",
        "model = hub.load(module_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcN3qzA60wWO"
      },
      "outputs": [],
      "source": [
        "# file management\n",
        "def open_file(filename):\n",
        "    read_file = open(filename, 'r')\n",
        "    file_cont = read_file.readlines()\n",
        "    read_file.close()\n",
        "\n",
        "    return file_cont\n",
        "\n",
        "def write_file(filename, content):\n",
        "    file_write = open(filename, 'w')\n",
        "    file_write.writelines(content)\n",
        "    file_write.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uBQxvbT0tzf"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyirWNWO1ZoT"
      },
      "outputs": [],
      "source": [
        "# sentences - list of (question, question + answer) tuples - from finetuning file\n",
        "# questions - list of (question, '') tuples - from questions file\n",
        "def create_sentences(finetuning_file):\n",
        "    ft = open_file(finetuning_file)\n",
        "    sentences = []\n",
        "    ques_ans = ''\n",
        "\n",
        "    for line in ft:\n",
        "        if 'P: ' in line:\n",
        "            ques_ans += line\n",
        "            ques = line\n",
        "\n",
        "        elif 'R: ' in line:\n",
        "            ques_ans += line\n",
        "\n",
        "            sentences.append((ques, ques_ans))\n",
        "            ques = ''\n",
        "            ques_ans = ''\n",
        "\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return list(sentences)\n",
        "\n",
        "def create_questions(questions_file):\n",
        "    ques = open_file(questions_file)\n",
        "    questions = []\n",
        "\n",
        "    for line in ques:\n",
        "        questions.append((line, ''))\n",
        "\n",
        "    return list(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEOl9yru1HZI"
      },
      "outputs": [],
      "source": [
        "# nearest neighbor function\n",
        "def display_nearest_neighbors(index, num_results, query_text, answer_text=None):\n",
        "    query_embedding = model.signatures['question_encoder'](tf.constant([query_text]))['outputs'][0]\n",
        "    search_results = index.nearest(query_embedding, n=num_results)\n",
        "    answer = ''\n",
        "\n",
        "    for s in search_results:\n",
        "        answer += s\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFzCPYMz1k6q"
      },
      "outputs": [],
      "source": [
        "# compute embeddings and build simple_neighbors index\n",
        "def compute_embeddings(sentences):\n",
        "    batch_size = 1\n",
        "\n",
        "    encodings = model.signatures['response_encoder'](input=tf.constant([sentences[0][0]]), context=tf.constant([sentences[0][1]]))\n",
        "    index = simpleneighbors.SimpleNeighbors(len(encodings['outputs'][0]), metric='angular')\n",
        "\n",
        "    print('Computing embeddings for %s sentences' % len(sentences))\n",
        "\n",
        "    slices = zip(*(iter(sentences),) * batch_size)\n",
        "    num_batches = int(len(sentences) / batch_size)\n",
        "\n",
        "    for s in tqdm(slices, total=num_batches):\n",
        "        response_batch = list([r for r, c in s])\n",
        "        context_batch = list([c for r, c in s])\n",
        "        encodings = model.signatures['response_encoder'](input=tf.constant(response_batch), context=tf.constant(context_batch))\n",
        "        \n",
        "        for batch_index, batch in enumerate(response_batch):\n",
        "            index.add_one(batch, encodings['outputs'][batch_index])\n",
        "\n",
        "    index.build()\n",
        "    print('simpleneighbors index for %s sentences built.' % len(sentences))\n",
        "\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve answer to given question\n",
        "def get_answer(question, evaluating_file):\n",
        "    answer = ''\n",
        "    eval = open_file(evaluating_file)\n",
        "\n",
        "    for i in range(len(eval)):\n",
        "        if eval[i] == question:\n",
        "            answer = eval[i + 1]\n",
        "    \n",
        "    return answer"
      ],
      "metadata": {
        "id": "hrUnLwpo6HHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8jCq7o3Wf6F"
      },
      "outputs": [],
      "source": [
        "def retrieve_results(domain_file_path, questions_file_path, evaluating_file_path, save_file_path):\n",
        "    results = []\n",
        "\n",
        "    sentences = create_sentences(domain_file_path)\n",
        "    questions = create_questions(questions_file_path)\n",
        "    print(\"%s sentences, %s questions extracted from dataset\" % (len(sentences), len(questions)))\n",
        "\n",
        "    index = compute_embeddings(sentences)\n",
        "    num_results = 1\n",
        "\n",
        "    for ques in questions:\n",
        "        similar_question = display_nearest_neighbors(index, num_results, ques[0], ques[1])\n",
        "\n",
        "        answer = get_answer(similar_question, evaluating_file_path)\n",
        "        aux_ques = 'P: ' + ques[0]\n",
        "        results.append(aux_ques)\n",
        "        results.append(answer)\n",
        "        results.append('\\n')\n",
        "\n",
        "    write_file(save_file_path, results)\n",
        "    print('File with posed questions and respective answers created!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTES\n",
        "\n",
        "# domain_file_path - path to the file containing the domain\n",
        "# must be a file containing question-answer pairs identified with 'P: ' and 'R: ', respectively\n",
        "# FAQs                      \n",
        "# P: question1             \n",
        "# R: answer1           \n",
        "# \\n                        \n",
        "# P: question2              \n",
        "# R: answer2                \n",
        "# \\n                       \n",
        "# must be a .txt file\n",
        "\n",
        "# questions_file_path - path to the file containing all questions, one question per line\n",
        "# Q1\n",
        "# Q2\n",
        "# Q3\n",
        "# ...\n",
        "# must be a .txt file\n",
        "\n",
        "# evaluating_file_path - path to the file containing all the questions to be posed and respective answers, to perform evaluation\n",
        "# must be a file containing question-answer pairs identified with 'P: ' and 'R: ', respectively\n",
        "# FAQs                      \n",
        "# P: question1             \n",
        "# R: answer1           \n",
        "# \\n                        \n",
        "# P: question2              \n",
        "# R: answer2                \n",
        "# \\n                       \n",
        "# must be a .txt file\n",
        "\n",
        "# save_file_path - path to the file where the posed questions and respective retrieved answers are to be saved\n",
        "# must be a .txt file"
      ],
      "metadata": {
        "id": "vkOEXkdoDbGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "domain_file_path = # 'domain_file_path'\n",
        "questions_file_path = # 'questions_file_path'\n",
        "evaluating_file_path = # 'evaluating_file_path'\n",
        "save_file_path = # 'save_file_path'\n",
        "\n",
        "retrieve_results(domain_file_path, questions_file_path, evaluating_file_path, save_file_path)"
      ],
      "metadata": {
        "id": "AUZLh6AeDeFS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "USE-QA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}