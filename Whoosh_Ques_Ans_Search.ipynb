{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Whoosh_Ques_Ans_Search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Whoosh - Search by Question Only or Question and Answer\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zx34GoRjRYSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# receives\n",
        "# .txt file containing a list of question-answer pairs identified by 'P: ' and 'R: '\n",
        "# .txt file with a list of questions, one question per line\n",
        "\n",
        "# retrieves\n",
        "# .txt file containing the posed questions and respective Whoosh's answers, identified by 'P: ' and 'R: '"
      ],
      "metadata": {
        "id": "-oMheQuERfPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7xEw2HMRObb"
      },
      "outputs": [],
      "source": [
        "pip install whoosh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh.index import create_in\n",
        "from whoosh.fields import *\n",
        "from whoosh.qparser import QueryParser\n",
        "from whoosh import qparser\n",
        "from whoosh.analysis import LanguageAnalyzer"
      ],
      "metadata": {
        "id": "6EIxSzuiR8nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file management\n",
        "def open_file(filename):\n",
        "    read_file = open(filename, 'r')\n",
        "    file_cont = read_file.readlines()\n",
        "    read_file.close()\n",
        "\n",
        "    return file_cont\n",
        "\n",
        "def write_file(filename, content):\n",
        "    file_write = open(filename, 'w')\n",
        "    file_write.writelines(content)\n",
        "    file_write.close()"
      ],
      "metadata": {
        "id": "-k0yeO0yR-V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FnSKUD7HSCtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be649ea2-c30e-4c66-e997-109c528ab91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get question and answer from question-answer pair file\n",
        "def get_ques_ans(file_path, ques_ans):\n",
        "    content = ''\n",
        "    file_content = open_file(file_path)\n",
        "\n",
        "    # return question\n",
        "    if ques_ans == 0: \n",
        "        content = file_content[ques_ans]\n",
        "\n",
        "    # return answer\n",
        "    elif ques_ans == 1:\n",
        "        for i in range(1, len(file_content)):\n",
        "            if file_content[i] == '\\n':\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                content = content +  file_content[i]\n",
        "    \n",
        "    return content"
      ],
      "metadata": {
        "id": "MtsFWohZTb8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates an index and writer objects, to add documents to be searched\n",
        "def create_index_writer(config, files_paths, files_names, ix_path):\n",
        "    \n",
        "    # default configuration\n",
        "    if config == 'default':\n",
        "        schema = Schema(title = TEXT(stored = True), \n",
        "        path = ID, \n",
        "        question = TEXT, \n",
        "        answer = TEXT(stored = True))\n",
        "\n",
        "    # portuguese language analyzer\n",
        "    elif config == 'lang_pt':\n",
        "        schema = Schema(title = TEXT(stored = True), \n",
        "        path = ID, \n",
        "        question = TEXT(analyzer=LanguageAnalyzer('pt')), \n",
        "        answer = TEXT(stored = True))\n",
        "\n",
        "    # n-gram filter(2-3)\n",
        "    elif config == 'ngram_3':\n",
        "        schema = Schema(title = TEXT(stored = True), \n",
        "        path = ID, \n",
        "        question = NGRAM(minsize = 2, maxsize = 3), \n",
        "        answer = TEXT(stored = True))\n",
        "\n",
        "    # n-gram filter(2-4)\n",
        "    elif config == 'ngram_4':\n",
        "        schema = Schema(title = TEXT(stored = True), \n",
        "        path = ID, \n",
        "        question = NGRAM(minsize = 2, maxsize = 4), \n",
        "        answer = TEXT(stored = True))   \n",
        "\n",
        "    # creates whoosh index\n",
        "    ix = create_in(ix_path, schema)\n",
        "\n",
        "    # creates writer object to add documents to be searched\n",
        "    writer = ix.writer()\n",
        "\n",
        "    # adds documents to writer object\n",
        "    for i in range(len(files_paths)):\n",
        "        # gets question from file with question-answer pair\n",
        "        aux_question = get_ques_ans(files_paths[i], 0)\n",
        "\n",
        "        # gets answer from file with question-answer pair\n",
        "        aux_answer = get_ques_ans(files_paths[i], 1)\n",
        "\n",
        "        # adds a document containing a question-answer pair to writer object\n",
        "        writer.add_document(title = files_names[i], path = files_paths[i], question = aux_question, answer = aux_answer)\n",
        "\n",
        "    writer.commit()\n",
        "\n",
        "    return ix"
      ],
      "metadata": {
        "id": "ZlMeG2KxSFCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieves the answer field of the document most similar to the posed question\n",
        "def most_similar_doc(question, search_type, ix):\n",
        "    answer = ''\n",
        "\n",
        "    with ix.searcher() as searcher:\n",
        "        og = qparser.OrGroup.factory(0.9)\n",
        "\n",
        "        # search only by the field question\n",
        "        if search_type == 'ques':\n",
        "            parser = qparser.QueryParser('question', ix.schema, group=og)\n",
        "\n",
        "        # search by both question and answer fields\n",
        "        elif search_type == 'quesans':\n",
        "            parser = qparser.MultifieldParser(['question', 'answer'], ix.schema, group=og)\n",
        "\n",
        "        # creates query and search objects and finds most similar document\n",
        "        query = parser.parse(question)\n",
        "        s = ix.searcher()\n",
        "        results = s.search(query, limit = 1)\n",
        "\n",
        "        if len(results) > 0:\n",
        "            result = results[0]\n",
        "            answer = result[\"answer\"]\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "zj0z-K7VV9ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieves a file with the posed questions and Whoosh's answers\n",
        "def scored_document_faqs(config, files_paths, files_names, ix_path, questions_file_path, search_type, save_file_path):\n",
        "    final_file_content = []\n",
        "    questions = open_file(questions_file_path)\n",
        "\n",
        "    # creates an index and writer objects, adds documents\n",
        "    ix = create_index_writer(config, files_paths, files_names, ix_path)\n",
        "\n",
        "    for ques in questions:\n",
        "        question = ques.replace('\"', '')\n",
        "\n",
        "        # most similar document's answer\n",
        "        answer = most_similar_doc(question, search_type, ix)\n",
        "\n",
        "        # creates the final file containing all posed questions and respective retrieved answers, with 'P: ' and 'R: ' identifiers\n",
        "        if answer != '':\n",
        "            aux_q = 'P: ' + question\n",
        "            final_file_content.append(aux_q)\n",
        "            final_file_content.append(answer)\n",
        "            final_file_content.append('\\n')\n",
        "\n",
        "            print(aux_q)\n",
        "            print(answer)\n",
        "            print('\\n')\n",
        "            \n",
        "    write_file(save_file_path, final_file_content)\n",
        "    print('File with posed questions and respective answers created!')"
      ],
      "metadata": {
        "id": "5sabNhXEXWe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# divides original domain file in multiple files, one question-answer pair per file\n",
        "def divide_faqs(domain_file_path, save_domain_path):\n",
        "    ques_ans_pair = []\n",
        "    count = 0\n",
        "    files_paths = []\n",
        "    files_names = []\n",
        "\n",
        "    faqs_list = open_file(domain_file_path)\n",
        "\n",
        "    for i in range(len(faqs_list)):\n",
        "        if 'P: ' in faqs_list[i]:\n",
        "            ques_ans_pair.append(faqs_list[i])\n",
        "        \n",
        "        if 'R: ' in faqs_list[i]:\n",
        "            for j in range(i, len(faqs_list)):\n",
        "                if faqs_list[j] == '\\n':\n",
        "                    break\n",
        "                else:\n",
        "                    ques_ans_pair.append(faqs_list[j])\n",
        "\n",
        "            count += 1\n",
        "            aux_file_name = 'FAQ' + str(count) + '.txt'\n",
        "            aux_file_path = save_domain_path + aux_file_name\n",
        "            files_paths.append(aux_file_path)\n",
        "            files_names.append(aux_file_name)\n",
        "\n",
        "            write_file(aux_file_path, ques_ans_pair)\n",
        "            ques_ans_pair = []\n",
        "\n",
        "    return files_paths, files_names"
      ],
      "metadata": {
        "id": "R59Ngqu-Z0e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTES\n",
        "\n",
        "# domain_file_path - path to the file containing the domain\n",
        "# must be a file containing question-answer pairs identified with 'P: ' and 'R: ', respectively\n",
        "# FAQs                      \n",
        "# P: question1             \n",
        "# R: answer1           \n",
        "# \\n                        \n",
        "# P: question2              \n",
        "# R: answer2                \n",
        "# \\n                       \n",
        "# must be a .txt file\n",
        "\n",
        "# save_domain_path - path where the files containing each one question-answer pair are to be saved\n",
        "# ix_path - path where the index object is to be saved\n",
        "\n",
        "# questions_file_path - path to the file containing all questions, one question per line\n",
        "# Q1\n",
        "# Q2\n",
        "# Q3\n",
        "# ...\n",
        "# must be a .txt file\n",
        "\n",
        "# save_file_path - path to the file where the posed questions and respective retrieved answers are to be saved\n",
        "# must be a .txt file\n",
        "\n",
        "# config can be: \n",
        "# 'default' - default whoosh configuration\n",
        "# 'lang_pt' - portuguese language analyzer - converts words to lower-case, removes Portuguese stopwords, and converts words to their stem, following Portuguese rules\n",
        "# 'ngram_3' - added n-gram filter(2-3)\n",
        "# 'ngram_4' - added n-gram filter(2-4)\n",
        "\n",
        "# search_type can be:\n",
        "# 'ques' - search by question field only\n",
        "# 'quesans' - search by both question and answer fields"
      ],
      "metadata": {
        "id": "ZPf4lFaCZeVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "domain_file_path = # 'domain_file_path'\n",
        "save_domain_path = # 'save_domain_path'\n",
        "\n",
        "ix_path = # 'ix_path'\n",
        "\n",
        "questions_file_path = # 'questions_file_path'\n",
        "save_file_path = # 'save_file_path'\n",
        "\n",
        "config = # 'default' or 'lang_pt' or 'ngram_3' or 'ngram_4'\n",
        "search_type = # 'ques' or 'quesans'\n",
        "\n",
        "(files_paths, files_names) = divide_faqs(domain_file_path, save_domain_path)\n",
        "scored_document_faqs(config, files_paths, files_names, ix_path, questions_file_path, search_type, save_file_path)"
      ],
      "metadata": {
        "id": "wUABjeoFZebh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}