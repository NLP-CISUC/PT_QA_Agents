{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# BERT - Clustering\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1_OLxLe-sxK7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDS-5NE6supH"
      },
      "outputs": [],
      "source": [
        "# receives\n",
        "# .txt file containing a list of question-answer pairs identified by 'P: ' and 'R: '\n",
        "# .txt file with a list of questions, one question per line\n",
        "\n",
        "# retrieves\n",
        "# .txt file containing the posed questions and respective Whoosh's answers, identified by 'P: ' and 'R: '"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n",
        "pip install pandas\n",
        "pip install sklearn\n",
        "pip install kneed"
      ],
      "metadata": {
        "id": "ohBfbGTms2CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import pipeline\n",
        "import numpy\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import kneed\n",
        "import math\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
        "from numpy import savetxt\n",
        "from numpy import loadtxt"
      ],
      "metadata": {
        "id": "doRMYZh-s41G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file management\n",
        "def open_file(filename):\n",
        "    read_file = open(filename, 'r')\n",
        "    file_cont = read_file.readlines()\n",
        "    read_file.close()\n",
        "\n",
        "    return file_cont\n",
        "\n",
        "def write_file(filename, content):\n",
        "    file_write = open(filename, 'w')\n",
        "    file_write.writelines(content)\n",
        "    file_write.close()"
      ],
      "metadata": {
        "id": "p95r6KW4s_HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VQNFPArns-GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT - feature extraction pipeline\n",
        "def bert_model_fe(model_name, pipeline_name):\n",
        "    pipe_feat_extraction = pipeline(pipeline_name, model=model_name)\n",
        "    return pipe_feat_extraction\n",
        "\n",
        "def get_vector(question, pipe_feat_extraction):\n",
        "    vector = pipe_feat_extraction(question)\n",
        "    result = vector[0][0]\n",
        "\n",
        "    return result\n",
        "\n",
        "pipe_feat_extraction = bert_model_fe('neuralmind/bert-large-portuguese-cased', \"feature-extraction\")\n",
        "print('Feature Extraction Model Downloaded')"
      ],
      "metadata": {
        "id": "vr8kwTWDtBVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT - question answering pipeline\n",
        "def bert_model_qa(model_name, pipeline_name):\n",
        "    pipe_ques_answering = pipeline(pipeline_name, model=model_name)\n",
        "    return pipe_ques_answering\n",
        "\n",
        "def get_answer_qa(context, question, pipe_ques_answering):\n",
        "    result = pipe_ques_answering(question=question, context=context)\n",
        "\n",
        "    return result\n",
        "\n",
        "pipe_ques_answering = bert_model_qa('pierreguillou/bert-base-cased-squad-v1.1-portuguese', \"question-answering\")\n",
        "print('Question Answering Model Downloaded')"
      ],
      "metadata": {
        "id": "FiHA-PUQtC1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns the answer corresponding to the question in position question_index of question-answer pairs file\n",
        "def get_answer(file_content, question_index):\n",
        "    answer = ''\n",
        "\n",
        "    for i in range(question_index + 1, len(file_content)):\n",
        "        if file_content[i] == '\\n' or file_content[i].contains('P: '):\n",
        "            break\n",
        "        else:\n",
        "            answer = answer + file_content[i]\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "54hFFniSwARX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns clusters and respective centroids after performing k-means clustering\n",
        "def km_cluster(data):\n",
        "    sum_squared_distances = []\n",
        "\n",
        "    # find optimal number of clusters\n",
        "    aux_k = range(1,100)\n",
        "    for num_clusters in aux_k:\n",
        "        print('Cluster ' + str(num_clusters))\n",
        "        kmeans = KMeans(n_clusters=num_clusters)\n",
        "        kmeans.fit(data)\n",
        "        sum_squared_distances.append(kmeans.inertia_)\n",
        "\n",
        "    best_k = kneed.KneeLocator(range(1, 100), sum_squared_distances, curve=\"convex\", direction=\"decreasing\")\n",
        "    print('Creating ' + str(best_k.elbow) + ' clusters.')\n",
        "\n",
        "    # perform k-means\n",
        "    kmeans = KMeans(best_k.elbow)\n",
        "    kmeans.fit(data)\n",
        "\n",
        "    # return clusters and centroids\n",
        "    identified_clusters = kmeans.fit_predict(data)\n",
        "    centroids  = kmeans.cluster_centers_\n",
        "\n",
        "    return identified_clusters, centroids"
      ],
      "metadata": {
        "id": "TZi-M2L1ttnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates files with clusters and centroids from k-means clustering\n",
        "def bert_clustering(domain_file_path, clustering_file_path, centroids_file_path, pipe_feat_extraction):\n",
        "    data = []\n",
        "    domain_content = open_file(domain_file_path)\n",
        "    \n",
        "    for line in domain_content:\n",
        "        if 'P: ' in line:\n",
        "            question = line.replace('P: ', '')\n",
        "            ques_vector = get_vector(question, pipe_feat_extraction)\n",
        "            data.append(ques_vector)\n",
        "\n",
        "    # k-means clustering, returns centroids and clusters\n",
        "    (identified_clusters, centroids) = km_cluster(data)\n",
        "\n",
        "    # save clusters and centroids to csv file\n",
        "    savetxt(clustering_file_path, identified_clusters, delimiter=',')\n",
        "    savetxt(centroids_file_path, centroids, delimiter=',')\n",
        "\n",
        "    print('Clusters Formed and Centroids Calculated!')"
      ],
      "metadata": {
        "id": "SiGM-T6EuU_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieves a file with the posed questions and BERT's answers\n",
        "def bert_clus_answers(clustering_file_path, centroids_file_path, domain_file_path, questions_file_path, saving_file_path, pipe_feat_extraction, pipe_ques_answering):\n",
        "    final_file_content = []\n",
        "    questions = open_file(questions_file_path)\n",
        "    faqs_list = open_file(domain_file_path)\n",
        "\n",
        "    # creates files with clusters and centroids from k-means clustering \n",
        "    bert_clustering(domain_file_path, clustering_file, centroids_file, pipe_feat_extraction)\n",
        "\n",
        "    # gets files with clusters and centroids from k-means clustering \n",
        "    identified_clusters = loadtxt(clustering_file_path, delimiter=',')\n",
        "    centroids = loadtxt(centroids_file_path, delimiter=',')\n",
        "    \n",
        "    for i in range(len(questions)):\n",
        "        cos = 0\n",
        "        context = []\n",
        "        centroid_id = -1\n",
        "\n",
        "        # create question embedding\n",
        "        ques_emb = get_vector(questions[i], pipe_feat_extraction)\n",
        "\n",
        "        # compare with centroids\n",
        "        for j in range(len(centroids)):\n",
        "\n",
        "            # calculate cosine between question and centroid\n",
        "            aux_centroids = centroids[j]\n",
        "            aux_cos = 1 - scipy.spatial.distance.cosine(centroids[j], ques_emb)\n",
        "\n",
        "            # if greater cosine than last, update centroid\n",
        "            if aux_cos >= cos:\n",
        "                cos = aux_cos\n",
        "\n",
        "                # closest centroid is the respective cluster number\n",
        "                centroid_id = j\n",
        "\n",
        "        # go to clusters array and save the positions of respective centroid faqs\n",
        "        faqs_pos = []\n",
        "        for m in range(len(identified_clusters)):\n",
        "            if math.ceil(identified_clusters[m]) == centroid_id:\n",
        "                faqs_pos.append(m)\n",
        "        \n",
        "        # form the context: from questions positions, grab question and answer in domain file\n",
        "        question_pos = 0\n",
        "        for n in range(len(faqs_list)):\n",
        "            if 'P: ' in faqs_list[n]:\n",
        "                for pos in faqs_pos:\n",
        "                    if question_pos == pos:\n",
        "                        context.append(faqs_list[n])\n",
        "                        aux_a = get_answer(faqs_list, n)\n",
        "                        context.append(aux_a)\n",
        "                        context.append('\\n')\n",
        "                        continue\n",
        "                question_pos += 1\n",
        "\n",
        "        # get BERT-QA answer, with cluster as context\n",
        "        str_context = format_context(context)\n",
        "        answer = get_answer_qa(str_context, questions[i], pipe_ques_answering)\n",
        "\n",
        "        # creates the final file containing all posed questions and respective retrieved answers, with 'P: ' and 'R: ' identifiers\n",
        "        aux_ques = 'P: ' + questions[i]\n",
        "        final_file_content.append(aux_ques)\n",
        "        aux_ans = 'R: ' + answer['answer']\n",
        "        final_file_content.append(aux_ans)\n",
        "        final_file_content.append('\\n\\n')\n",
        "\n",
        "    write_file(saving_file_path, final_file_content)\n",
        "    print('File with BERTs FAQs answers to questions with clustering created!')"
      ],
      "metadata": {
        "id": "N_18X16_tfwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTES\n",
        "\n",
        "# domain_file_path - path to the file containing the domain\n",
        "# must be a file containing question-answer pairs identified with 'P: ' and 'R: ', respectively\n",
        "# FAQs                      \n",
        "# P: question1             \n",
        "# R: answer1           \n",
        "# \\n                        \n",
        "# P: question2              \n",
        "# R: answer2                \n",
        "# \\n                       \n",
        "# must be a .txt file\n",
        "\n",
        "# questions_file_path - path to the file containing all questions, one question per line\n",
        "# Q1\n",
        "# Q2\n",
        "# Q3\n",
        "# ...\n",
        "# must be a .txt file\n",
        "\n",
        "# save_file_path - path to the file where the posed questions and respective retrieved answers are to be saved\n",
        "# must be a .txt file\n",
        "\n",
        "# clustering_file_path - path to the file where the clustering results are to be saved\n",
        "# must be a .txt file\n",
        "\n",
        "# centroids_file_path - path to the file where the centroids are to be saved\n",
        "# must be a .txt file"
      ],
      "metadata": {
        "id": "nlZ4jtLVxE7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "domain_file_path = # 'domain_file_path'\n",
        "questions_file_path = # 'questions_file_path'\n",
        "save_file_path = # 'save_file_path'\n",
        "\n",
        "clustering_file_path = # 'clustering_file_path'\n",
        "centroids_file_path = # 'centroids_file_path'\n",
        "\n",
        "bert_clus_answers(clustering_file_path, centroids_file_path, domain_file_path, questions_file_path, saving_file_path, pipe_feat_extraction, pipe_ques_answering)"
      ],
      "metadata": {
        "id": "jSoxcBR6xE9-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}