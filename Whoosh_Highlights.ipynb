{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Whoosh_Highlights.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Whoosh - Highlights\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SgyQvJaWeTKS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFIen6rwd8q5"
      },
      "outputs": [],
      "source": [
        "# receives\n",
        "# a list of .txt files containing raw text\n",
        "# .txt file with a list of questions, one question per line\n",
        "\n",
        "# retrieves\n",
        "# .txt file containing the posed questions and respective Whoosh's answers, identified by 'P: ' and 'R: '"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install whoosh"
      ],
      "metadata": {
        "id": "F8b0inl3ekZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22c631b-4909-49f4-8e14-bf1fbb98bdc2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 2.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: whoosh\n",
            "Successfully installed whoosh-2.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh.index import create_in\n",
        "from whoosh.fields import *\n",
        "from whoosh.qparser import QueryParser\n",
        "from whoosh import qparser\n",
        "from whoosh.analysis import LanguageAnalyzer\n",
        "from whoosh import highlight"
      ],
      "metadata": {
        "id": "OkSCrO7Jekss"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file management\n",
        "def open_file(filename):\n",
        "    read_file = open(filename, 'r')\n",
        "    file_cont = read_file.readlines()\n",
        "    read_file.close()\n",
        "\n",
        "    return file_cont\n",
        "\n",
        "def write_file(filename, content):\n",
        "    file_write = open(filename, 'w')\n",
        "    file_write.writelines(content)\n",
        "    file_write.close()"
      ],
      "metadata": {
        "id": "BYBNAuHMem7C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "80GOlsO8eofK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get content of file\n",
        "def get_content(file_path):\n",
        "    content = ''\n",
        "    file_cont = open_file(file_path)\n",
        "\n",
        "    for i in range(len(file_cont)):\n",
        "        content = content + file_cont[i]\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "biXrlZayhmJD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates an index and writer objects, to add documents to be searched\n",
        "def create_index_writer(config, files_paths, files_names, ix_path):\n",
        "\n",
        "    # default configuration\n",
        "    if config == 'default':\n",
        "        schema = Schema(title = TEXT(stored = True), \n",
        "        path = ID, \n",
        "        content = TEXT(stored = True))\n",
        "\n",
        "    # portuguese language analyzer\n",
        "    elif config == 'lang_pt':\n",
        "        schema = Schema(title = TEXT(stored = True), \n",
        "        path = ID, \n",
        "        content = TEXT(analyzer=LanguageAnalyzer('pt'), stored = True))\n",
        "\n",
        "    # n-gram filter(2-3)\n",
        "    elif config == 'ngram_3':\n",
        "        schema = Schema(title = TEXT(stored = True), \n",
        "        path = ID, \n",
        "        content = NGRAM(minsize = 2, maxsize = 3, stored = True))\n",
        "\n",
        "    # n-gram filter(2-4)\n",
        "    elif config == 'ngram_4':\n",
        "        schema = Schema(title = TEXT(stored = True), \n",
        "        path = ID, \n",
        "        content = NGRAM(minsize = 2, maxsize = 4, stored = True)) \n",
        "\n",
        "    # creates whoosh index\n",
        "    ix = create_in(ix_path, schema)\n",
        "\n",
        "    # creates writer object to add documents to be searched\n",
        "    writer = ix.writer()\n",
        "\n",
        "    # adds documents to writer object\n",
        "    for i in range(len(files_names)):\n",
        "\n",
        "        # get content from file\n",
        "        aux_content = get_content(files_paths[i])\n",
        "\n",
        "        # adds a document containing the content of a text file\n",
        "        writer.add_document(title = files_names[i], path = files_paths[i], content = aux_content)\n",
        "    writer.commit()\n",
        "\n",
        "    return ix"
      ],
      "metadata": {
        "id": "X8l13blsepsT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieves the highlights of the document most similar to the posed question\n",
        "def most_similar_doc(ix, question):\n",
        "    answer = ''\n",
        "\n",
        "    with ix.searcher() as searcher:\n",
        "\n",
        "        # creates query and search objects and finds most similar document\n",
        "        og = qparser.OrGroup.factory(0.9)\n",
        "        parser = qparser.QueryParser('content', ix.schema, group=og)\n",
        "        query = parser.parse(question)\n",
        "\n",
        "        s = ix.searcher()\n",
        "        results = s.search(query, limit = 1)\n",
        "\n",
        "        results.formatter = highlight.NullFormatter()\n",
        "        aux_result = results[0].highlights('content')\n",
        "        answer = aux_result.replace('\\n\\n', '\\n')\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "h-f_alOCiyG9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieves a file with the posed questions and Whoosh's answers\n",
        "def scored_document(config, files_paths, files_names, ix_path, questions_file_path, save_file_path):\n",
        "    final_file_content = []\n",
        "    questions = open_file(questions_file_path)\n",
        "\n",
        "    ix = create_index_writer(config, files_paths, files_names, ix_path)\n",
        "\n",
        "    for question in questions:\n",
        "\n",
        "        # most similar document's answer\n",
        "        answer = most_similar_doc(ix, question)\n",
        "\n",
        "        # creates the final file containing all posed questions and respective retrieved answers, with 'P: ' and 'R: ' identifiers\n",
        "        aux_q = 'P: ' + question\n",
        "        aux_a = 'R: ' + answer\n",
        "        final_file_content.append(aux_q)\n",
        "        final_file_content.append(aux_a)\n",
        "        final_file_content.append('\\n\\n')\n",
        "    \n",
        "    write_file(save_file_path, final_file_content)\n",
        "    print('File with posed questions and respective answers created!')"
      ],
      "metadata": {
        "id": "tTtqKY2_kma2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTES\n",
        "\n",
        "# files_paths - list with paths of files in domain\n",
        "# files_names - list with names of files in domain\n",
        "# position of each list must match\n",
        "\n",
        "# must be a collection of files containing raw text\n",
        "# Text\n",
        "# Paragraph1Line1\n",
        "# Paragraph1Line2\n",
        "# \\n                       \n",
        "# Paragraph2Line1\n",
        "# Paragraph2Line2\n",
        "# \\n                        \n",
        "# must be a .txt file                     \n",
        "\n",
        "# ix_path - path where the index object is to be saved\n",
        "\n",
        "# questions_file_path - path to the file containing all questions, one question per line\n",
        "# Q1\n",
        "# Q2\n",
        "# Q3\n",
        "# ...\n",
        "# must be a .txt file\n",
        "\n",
        "# save_file_path - path to the file where the posed questions and respective retrieved answers are to be saved\n",
        "# must be a .txt file\n",
        "\n",
        "# config can be: \n",
        "# 'default' - default whoosh configuration\n",
        "# 'lang_pt' - portuguese language analyzer - converts words to lower-case, removes Portuguese stopwords, and converts words to their stem, following Portuguese rules\n",
        "# 'ngram_3' - added n-gram filter(2-3)\n",
        "# 'ngram_4' - added n-gram filter(2-4)"
      ],
      "metadata": {
        "id": "Z6DReWHCls2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_paths = # ['file_path_1', 'file_path_2']\n",
        "files_names = # ['files_name_1', 'files_name_2']\n",
        "\n",
        "ix_path = # 'ix_path'\n",
        "questions_file_path = # 'questions_file_path'\n",
        "save_file_path = # 'save_file_path'\n",
        "\n",
        "config = # 'default' or 'lang_pt' or 'ngram_3' or 'ngram_4'\n",
        "\n",
        "scored_document(config, files_paths, files_names, ix_path, questions_file_path, save_file_path)"
      ],
      "metadata": {
        "id": "f5fYFWmbmCW1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}